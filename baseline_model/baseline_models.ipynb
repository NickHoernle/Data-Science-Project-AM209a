{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC209a Milestone 4: Baseline Recommendation Models\n",
    "\n",
    "**Andrew Ross, Sophie Hilgard, Reiko Nishihara, Nick Hoernle**\n",
    "\n",
    "In this notebook, we'll try various baseline models for recommendation and compare their performance.\n",
    "\n",
    "We'll mainly use root mean squared error as our performance metric, in part because it's a good metric (interpretable, has units of stars), and in part because that's what most other recommendation systems, papers we found, and even software packages use to measure performance, so we can more easily compare our models to state of the art benchmarks.\n",
    "\n",
    "First let's import packages and load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../util')\n",
    "from load_yelp_data import load_yelp_dataframe, restaurants_and_bars_in, train_test_split_reviews\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import graphlab as gl\n",
    "import sklearn.metrics\n",
    "%matplotlib inline\n",
    "\n",
    "from matrix_factorization_recommender import MatrixFactorizationRecommender\n",
    "from simple_averaging_recommender import SimpleAveragingRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "businesses = load_yelp_dataframe('businesses')\n",
    "reviews = load_yelp_dataframe('reviews')\n",
    "users = load_yelp_dataframe('users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought it would be interesting if we tried limiting our data to just restaurants and bars in Phoenix (and for comparison, Las Vegas) to support some fine-grained location-based analysis that we're still thinking about. So let's just look at Phoenix for the remainder of this notebook, maybe jumping back to compare Phoenix RMSEs to global RMSEs for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoenix_restaurants, phoenix_reviews, phoenix_users = restaurants_and_bars_in('Phoenix', businesses, reviews, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>full_address</th>\n",
       "      <th>hours</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhoods</th>\n",
       "      <th>open</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>{'Accepts Credit Cards': True, 'Noise Level': ...</td>\n",
       "      <td>2377</td>\n",
       "      <td>['Sandwiches', 'Pizza', 'Chicken Wings', 'Rest...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2819 N Central Ave\\nPhoenix, AZ 85004</td>\n",
       "      <td>{'Monday': {'close': '00:00', 'open': '10:00'}...</td>\n",
       "      <td>33.479482</td>\n",
       "      <td>-112.073681</td>\n",
       "      <td>Domino's Pizza</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>business</td>\n",
       "      <td>Phoenix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>{'Accepts Credit Cards': True, 'Noise Level': ...</td>\n",
       "      <td>2378</td>\n",
       "      <td>['American (New)', 'Sandwiches', 'Restaurants']</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>1850 N Central Ave\\nPhoenix, AZ 85004</td>\n",
       "      <td>{}</td>\n",
       "      <td>33.468547</td>\n",
       "      <td>-112.075085</td>\n",
       "      <td>Viad Tower Restaurants</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>AZ</td>\n",
       "      <td>business</td>\n",
       "      <td>Phoenix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             attributes  business_id  \\\n",
       "2377  {'Accepts Credit Cards': True, 'Noise Level': ...         2377   \n",
       "2378  {'Accepts Credit Cards': True, 'Noise Level': ...         2378   \n",
       "\n",
       "                                             categories     city  \\\n",
       "2377  ['Sandwiches', 'Pizza', 'Chicken Wings', 'Rest...  Phoenix   \n",
       "2378    ['American (New)', 'Sandwiches', 'Restaurants']  Phoenix   \n",
       "\n",
       "                               full_address  \\\n",
       "2377  2819 N Central Ave\\nPhoenix, AZ 85004   \n",
       "2378  1850 N Central Ave\\nPhoenix, AZ 85004   \n",
       "\n",
       "                                                  hours   latitude  \\\n",
       "2377  {'Monday': {'close': '00:00', 'open': '10:00'}...  33.479482   \n",
       "2378                                                 {}  33.468547   \n",
       "\n",
       "       longitude                    name neighborhoods  open  review_count  \\\n",
       "2377 -112.073681          Domino's Pizza            []  True            20   \n",
       "2378 -112.075085  Viad Tower Restaurants            []  True             6   \n",
       "\n",
       "      stars state      type location  \n",
       "2377    3.0    AZ  business  Phoenix  \n",
       "2378    3.5    AZ  business  Phoenix  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoenix_restaurants.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def utility_matrix_fullness(reviews):\n",
    "    return len(reviews) / float(len(reviews.business_id.unique()) * len(reviews.user_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall utility matrix fullness: 0.0046%\n",
      "Phoenix utility matrix fullness: 0.0374%\n"
     ]
    }
   ],
   "source": [
    "print 'Overall utility matrix fullness: {:.4%}'.format(utility_matrix_fullness(reviews))\n",
    "print 'Phoenix utility matrix fullness: {:.4%}'.format(utility_matrix_fullness(phoenix_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So limiting to just restaurants and bars in Phoenix makes our utility matrix less sparse by about an order of magnitude. That makes sense intuitively, because we expect the network of restaurants and reviewers to be more highly connected when all the reviewers and restaurants are in the same physical location, and in addition to allowing for more interesting side-analyses, it should hopefully also improve the accuracy of our models.\n",
    "\n",
    "Let's split our Phoenix restaurant/bar data into a training and test set, and evaluate the performance of various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def X_and_y_of(df):\n",
    "    return df[['user_id', 'business_id']].values, df['stars'].values\n",
    "\n",
    "reviews_train, reviews_test = train_test_split_reviews(phoenix_reviews)\n",
    "\n",
    "X_train, y_train = X_and_y_of(reviews_train)\n",
    "X_test, y_test = X_and_y_of(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_performance(model):\n",
    "    name = model.__class__.__name__\n",
    "    print '{} train RMSE: {}'.format(name, model.rmse(X_train, y_train))\n",
    "    print '{} test RMSE: {}'.format(name, model.rmse(X_test, y_test))\n",
    "    print '{} train R^2: {}'.format(name, model.r2_score(X_train, y_train))\n",
    "    print '{} test R^2: {}'.format(name, model.r2_score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Averaging \n",
    "The first baseline model we want to try is simple averaging (whose code you can see [here](https://github.com/NickHoernle/Data-Science-Project-AM209a/blob/master/util/simple_averaging_recommender.py)). All that simple averaging does is compute the average rating across the entire dataset as well as average deviations from the global mean for each user and each business. Then for any new user, business pair, it predicts that just the global mean plus the user and business-specific baselines. If we haven't seen a particular user or business in our training set, we predict its baseline to equal 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleAveragingRecommender train RMSE: 0.976644727132\n",
      "SimpleAveragingRecommender test RMSE: 1.34699149129\n",
      "SimpleAveragingRecommender train R^2: 0.477786155479\n",
      "SimpleAveragingRecommender test R^2: 0.0102154548537\n"
     ]
    }
   ],
   "source": [
    "simple_avg = SimpleAveragingRecommender()\n",
    "simple_avg.fit(X_train, y_train)\n",
    "summarize_performance(simple_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these results make sense; we do a fair amount better on the training set than the test set,\n",
    "but our test RMSE isn't that bad. Our test R^2 is close to 0 though slightly above it, which makes\n",
    "sense for an average.\n",
    "\n",
    "In [this citation](http://cs229.stanford.edu/proj2014/Chee%20Hoon%20Ha,%20Yelp%20Recommendation%20System%20Using%20Advanced%20Collaborative%20Filtering.pdf)\n",
    "(which is another exploration of recommendation algorithms applied to the Yelp dataset),\n",
    "they report an RMSE of 1.47 for simple averaging. If we try applying our\n",
    "simple averaging to the global dataset (with shuffle-split cross-validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local RMSE 1.34745239586, global RMSE 1.36724780709\n"
     ]
    }
   ],
   "source": [
    "global_rmses = []\n",
    "local_rmses = []\n",
    "\n",
    "for _ in range(10):\n",
    "    reviews_train_global, reviews_test_global = train_test_split_reviews(reviews, seed=None)\n",
    "    reviews_train_local, reviews_test_local = train_test_split_reviews(phoenix_reviews, seed=None)\n",
    "    simple_avg.fit(*X_and_y_of(reviews_train_global))\n",
    "    global_rmses.append(simple_avg.rmse(*X_and_y_of(reviews_test_global)))\n",
    "    simple_avg.fit(*X_and_y_of(reviews_train_local))\n",
    "    local_rmses.append(simple_avg.rmse(*X_and_y_of(reviews_test_local)))\n",
    "    \n",
    "print 'Local RMSE {}, global RMSE {}'.format(np.mean(local_rmses), np.mean(global_rmses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, although our global RMSE is worse than our local RMSE (which we'd expect), we still do better than most of the citations we found just with simple averaging.\n",
    "\n",
    "### Matrix Factorization\n",
    "\n",
    "The second baseline model we tried was based on [Koren, Bell, and Volinsky's work](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) on latent factor models realized using matrix factorization techniques. In this framing of the problem, we assume that the full utility matrix of all users' ratings for all businesses, which we don't actually have (and has $|B|\\cdot|U|$ entries, where $|B|$ is the number of businesses and $|U|$ the number of users), is nevertheless explainable using a much smaller number of latent factors, which are essentially user and business vectors of dimension $f$ whose inner product generates the predicted rating. So the idea is that if we can find skinny $|B| \\times f$ and $f \\times |U|$ matrices that do a good job generating the ratings we do have in our very incomplete utility matrix, perhaps with some additional regularizing assumptions, then we can approximate those latent factors and make good predictions about ratings we're still missing.\n",
    "\n",
    "In practice, to find such matrices, we need to use optimization techniques on each of the individual elements on the matrices, and the algorithms get complicated. However, there are at least a few existing software packages which implement those algorithms, including [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit) (VW). We found an [example](https://github.com/JohnLangford/vowpal_wabbit/wiki/Matrix-factorization-example) of how to use VW to run matrix factorizations, and so following that, we wrote a very minimal [Python wrapper](https://github.com/NickHoernle/Data-Science-Project-AM209a/blob/master/util/matrix_factorization_recommender.py) around just that matrix factorization routine. As in the example, we let the number of latent factors per user/business $f$ be 10, we use L2 regularization, and we set the same learning rate parameters for the optimization process. It takes a few minutes to train, but the results definitely beat simple averaging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already exists, skipping vowpal-wabbit fitting\n",
      "MatrixFactorizationRecommender train RMSE: 1.19438634692\n",
      "MatrixFactorizationRecommender test RMSE: 1.19719273998\n",
      "MatrixFactorizationRecommender train R^2: 0.218975203111\n",
      "MatrixFactorizationRecommender test R^2: 0.218121785367\n"
     ]
    }
   ],
   "source": [
    "matrix_fac = MatrixFactorizationRecommender()\n",
    "matrix_fac.fit(X_train, y_train)\n",
    "summarize_performance(matrix_fac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's especially nice about this is that our test RMSE/R^2 is almost as good as our train RMSE/R^2, despite them being totally separate. This may indicate that the matrix factorization method is more robust to certain kinds of overfitting than other recommendation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods\n",
    "\n",
    "We also looked at several recommendation algorithms included in a library called [GraphLab Create](https://turi.com/products/create/docs/graphlab.toolkits.recommender.html), which is kind of a higher-powered though slightly more proprietary competitor to scikit-learn. We haven't explored all of the recommendation algorithms in this library fully yet, but they include techniques akin to simple averaging, matrix factorization, and collaborative filtering. To use it, first we need to load our data into a graphlab \"SFrame\" (GraphLab's scalable, sometimes S3-backed version of a data frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1480273879.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to andrew_ross@g.harvard.edu and will expire on November 23, 2017.\n"
     ]
    }
   ],
   "source": [
    "train_sframe = gl.SFrame(reviews_train[['business_id', 'user_id', 'stars']])\n",
    "test_sframe = gl.SFrame(reviews_test[['business_id', 'user_id', 'stars']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use it to train recommendation models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Recsys training: model = popularity</pre>"
      ],
      "text/plain": [
       "Recsys training: model = popularity"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Recsys training: model = item_similarity</pre>"
      ],
      "text/plain": [
       "Recsys training: model = item_similarity"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Recsys training: model = factorization_recommender</pre>"
      ],
      "text/plain": [
       "Recsys training: model = factorization_recommender"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Recsys training: model = ranking_factorization_recommender</pre>"
      ],
      "text/plain": [
       "Recsys training: model = ranking_factorization_recommender"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pop_model = gl.popularity_recommender.create(\n",
    "    train_sframe, user_id='user_id', item_id='business_id', target='stars', verbose=False)\n",
    "sim_model = gl.item_similarity_recommender.create(\n",
    "    train_sframe, user_id='user_id', item_id='business_id', target='stars', verbose=False)\n",
    "fac_model = gl.factorization_recommender.create(\n",
    "    train_sframe, user_id='user_id', item_id='business_id', target='stars', verbose=False)\n",
    "rank_fac_model = gl.ranking_factorization_recommender.create(\n",
    "    train_sframe, user_id='user_id', item_id='business_id', target='stars', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then evaluate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PopularityRecommender train RMSE: 1.22558628247\n",
      "PopularityRecommender test RMSE: 1.26124521478\n",
      "ItemSimilarityRecommender train RMSE: 3.97754696406\n",
      "ItemSimilarityRecommender test RMSE: 3.97934964672\n",
      "FactorizationRecommender train RMSE: 0.386348157547\n",
      "FactorizationRecommender test RMSE: 1.44804211375\n",
      "RankingFactorizationRecommender train RMSE: 0.391552052851\n",
      "RankingFactorizationRecommender test RMSE: 2.10284057685\n"
     ]
    }
   ],
   "source": [
    " for model in [pop_model, sim_model, fac_model, rank_fac_model]:\n",
    "    name = model.__class__.__name__\n",
    "    train_rmse = model.evaluate_rmse(train_sframe, target='stars')['rmse_overall']\n",
    "    test_rmse = model.evaluate_rmse(test_sframe, target='stars')['rmse_overall']\n",
    "    print '{} train RMSE: {}'.format(name, train_rmse)\n",
    "    print '{} test RMSE: {}'.format(name, test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the [Popularity Recommender](https://turi.com/products/create/docs/generated/graphlab.recommender.popularity_recommender.PopularityRecommender.html),\n",
    "which scores items according to the number of times they're seen in the training set, actually does better than our simple averaging model. Because the library isn't open-source, and the documentation for it a little bit sparse, it's hard to figure out\n",
    "exactly how it works, but the documentation does state that model that makes the same prediction for every user. To be fair, many Yelp users only review a single business, so perhaps such a model makes sense. In any case, this will definitely be a good baseline to compare against.\n",
    "\n",
    "The matrix factorization-based recommenders (in particular the [Factorization Recommender](https://turi.com/products/create/docs/generated/graphlab.recommender.factorization_recommender.FactorizationRecommender.html))\n",
    "appear to overfit the training set slightly, but there are many options for regularization and optimization parameters, so we can almost certainly improve them.\n",
    "\n",
    "The [Item Similarity Recommender](https://turi.com/products/create/docs/generated/graphlab.recommender.item_similarity_recommender.ItemSimilarityRecommender.html)\n",
    "is more along the lines of collaborative filtering, but because we aren't using any other attributes of businesses or users yet,\n",
    "it's not a fair baseline (as evidenced by its terrible RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "We were able to train fairly strong baseline models of several kinds -- simple averaging, user-agnostic popularity, and matrix factorization -- all without considering any of the attributes of our data, apart from how highly who rated what. Hopefully we can achieve stronger results when we try collaborative filtering or combining any of the above methods with time-sensitive baselines as described in the Netflix result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
