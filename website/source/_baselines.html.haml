%h2 Baseline Predictors
As a prerequisite to generating personalized recommendations, we first investigate the calculation of baseline predictors. These are predictions for each user and each business that are meant to represent the inherent criticality of the user and quality of the business. The calculation of these predictors should help normalize the data for use in sophisticated algorithms later as well as lead to better predictions for inactive or new users.
.temporary-spacer

%section#baselines-global-avg
  %h3 Always Predict The Mean
  The most simple baseline is to just predict the mean rating across all users and all businesses in the training set for each rating in the test set. This results in an RMSE of 1.354.
  =image_tag ('images/justmean.png')
  .temporary-spacer

%section#baselines-simple-avg
  %h3 Simple Averaging
  In this method, we simply compute the baseline for each restaurant or user to be its average deviation from the mean rating.
  =image_tag ('images/simpleaveraging.png')
  .temporary-spacer

%section#baselines-beta-prior
  %h3 The Beta Prior Method
  In the Beta prior method, the baseline for each restaurant or user is given according to a beta distribution where the two parameters can be roughly interpreted as "upvotes" and "downvotes". That is, high ratings given by a user or to a business will add to the magnitude of the first parameter, and low ratings given by a user or to a business will decrease the magnitude of the second parameter.
  =image_tag ('images/betaprior.png')
  .temporary-spacer

%section#baselines-decoupling
  %h3 Decoupled Regularized Averaging
  This is the primary baseline method used in the BellKor Netflix Prize solution. In this method, we first calculate the value of the baseline for a given restaurant by computing the regularized average of the deviation from mean rating for that restaurant. Then when computing similar regularized baseline values for each user we additionally subtract out the baselines we have already calculated for each restaurant in addition to the mean across all ratings. This decoupling process allows us to more accurately model specific user-restaurant interaction.
  .temporary-spacer

%section#baselines-leastsq-l2
  %h3 Least-Squares and the L2 Norm
  Similar to the above but perhaps even more accurate, we can solve a least squares equation wherein we seek to minimize the sum over all ratings of [rating(user, restaurant) - mean over all ratings - the baselines for the user and rating in question] while using a regularization term to push the values of the baselines toward zero.
  .temporary-spacer

%section#baselines-time-based
  %h3 Time-Based Baselines
  In the final Netflix paper, BellKor create time-dependent baselines wherein they bin ratings based on recency and then assign bin-specific terms for each item (in their case, movies; in ours, restaurants). Intuitively, this makes sense, as restaurants and movies tend to come in and out of fashion. However, we believe that the effect is much less pronounced for restaurants than it is for items on Netflix, which are almost certainly heavily featured when they are first added and are typically only consumed a single time. In the case of the Yelp data, we were unable to improve the baseline significantly from the original Decoupled Regularized Averaging, even after adding terms related to days of the week and reviews given in the same day, which we had observed elsewhere in our data exploration were significant factors in skewing ratings.
  .temporary-spacer
